{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Saber ( S equence A nnotator for B iomedical E ntities and R elations) is a deep-learning based tool for information extraction in the biomedical domain. The neural network model used is a BiLSTM-CRF [ 1 , 2 ]; a state-of-the-art architecture for sequence labelling. The model is implemented using Keras . The goal is that Saber will eventually perform all the important steps in text-mining of biomedical literature: Coreference resolution ( ) Biomedical named entity recognition (BioNER) ( ) Entity linking / grounding / normalization ( ) Simple relation extraction ( ) Event extraction ( ) Pull requests are welcome! If you encounter any bugs, please open an issue in the GitHub repository .","title":"Home"},{"location":"#home","text":"Saber ( S equence A nnotator for B iomedical E ntities and R elations) is a deep-learning based tool for information extraction in the biomedical domain. The neural network model used is a BiLSTM-CRF [ 1 , 2 ]; a state-of-the-art architecture for sequence labelling. The model is implemented using Keras . The goal is that Saber will eventually perform all the important steps in text-mining of biomedical literature: Coreference resolution ( ) Biomedical named entity recognition (BioNER) ( ) Entity linking / grounding / normalization ( ) Simple relation extraction ( ) Event extraction ( ) Pull requests are welcome! If you encounter any bugs, please open an issue in the GitHub repository .","title":"Home"},{"location":"guide_to_saber_api/","text":"Guide to the Saber API You can interact with Saber as a web-service (explained in Quick start ), command line tool, python package, or via the Juypter notebooks. If you created a virtual environment, remember to activate it first . Command line tool Currently, the command line tool simply trains the model. To use it, call (saber) $ python -m saber.train along with any command line arguments. For example, to train the model on the NCBI Disease corpus (saber) $ python -m saber.train --dataset_folder NCBI_disease_BIO See Resources for help preparing datasets for training. Run python -m saber.train -h to see all possible arguments. Of course, supplying arguments at the command line can quickly become cumbersome. Saber also allows you to specify a configuration file, which can be specified like so (saber) $ python -m saber.train --config_filepath path/to/config.ini Copy the contents of the default config file to a new *.ini file in order to get started. Note that arguments supplied at the command line overwrite those found in the configuration file. For example (saber) $ python -m saber.train --dataset_folder path/to/dataset --k_folds 10 would overwrite the arguments for dataset_folder and k_folds found in the configuration file. Python package You can also import Saber and interact with it as a python package. Saber exposes its functionality through the SequenceProcessor class. Here is just about everything Saber does in one script: from saber.sequence_processor import SequenceProcessor # First, create a SequenceProcessor object, which exposes Sabers functionality sp = SequenceProcessor () # Load a dataset and create a model (provide a list of datasets to use multi-task learning!) sp . load_dataset ( 'path/to/datasets/GENIA' ) sp . create_model () # Train and save a model sp . fit () sp . save ( 'pretrained_models/GENIA' ) # Load a model del sp sp = SequenceProcessor () sp . load ( 'pretrained_models/GENIA' ) # Perform prediction on raw text, get resulting annotation raw_text = 'The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53.' annotation = sp . annotate ( raw_text ) # Use transfer learning to continue training on a new dataset sp . load_dataset ( 'path/to/datasets/CRAFT' ) sp . fit () Transfer learning Transfer learning is as easy as training, saving, loading, and then continuing training of a model. Here is an example # Create and train a model on GENIA corpus sp = SequenceProcessor () sp . load_dataset ( 'path/to/datasets/GENIA' ) sp . create_model () sp . fit () sp . save ( 'pretrained_models/GENIA' ) # Load that model del sp sp = SequenceProcessor () sp . load ( 'pretrained_models/GENIA' ) # Use transfer learning to continue training on a new dataset sp . load_dataset ( 'path/to/datasets/CRAFT' ) sp . fit () Note that there is currently no way to easily do this with the command line interface, but I am working on it! Multi-task learning Multi-task learning is as easy as specifying multiple dataset paths, either in the config file, at the command line via the flag --dataset_folder , or as an argument to load_dataset() . The number of datasets is arbitrary. Here is an example using the last method sp = SequenceProcessor () # Simply pass multiple dataset paths as a list to load_dataset to use multi-task learning. sp . load_dataset ([ 'path/to/datasets/NCBI-Disease' , 'path/to/datasets/Linnaeus' ]) sp . create_model () sp . fit () Saving and loading models In the following sections we introduce the saving and loading of models. Saving a model Assuming the model has already been created (see above), we can easily save our model like so path_to_saved_model = 'path/to/pretrained_models/mymodel' sp . save ( path_to_saved_model ) Loading a model Lets illustrate loading a model with a new SequenceProccesor object # Delete our previous SequenceProccesor object (if it exists) if 'sp' in locals (): del sp # Create a new SequenceProccesor object sp = SequenceProcessor () # Load a previous model sp . load ( path_to_saved_model ) Juypter notebooks First, grab the notebook. Go here , then press command / control + s , and save the notebook as lightning_tour.ipynb somewhere on your computer. Next, install JupyterLab by following the instructions here . Once installed, run: (saber) $ jupyter lab A new window will open in your browser. Use it to search for lightning_tour.ipynb on your computer. A couple of notes: If you activated a virtual environment, \" myenv \", make sure you see Python [venv:myenv] in the top right of the Jupyter notebook. If you are using conda, you need to run conda install nb_conda with your environment activated.","title":"Guide to the Saber API"},{"location":"guide_to_saber_api/#guide-to-the-saber-api","text":"You can interact with Saber as a web-service (explained in Quick start ), command line tool, python package, or via the Juypter notebooks. If you created a virtual environment, remember to activate it first .","title":"Guide to the Saber API"},{"location":"guide_to_saber_api/#command-line-tool","text":"Currently, the command line tool simply trains the model. To use it, call (saber) $ python -m saber.train along with any command line arguments. For example, to train the model on the NCBI Disease corpus (saber) $ python -m saber.train --dataset_folder NCBI_disease_BIO See Resources for help preparing datasets for training. Run python -m saber.train -h to see all possible arguments. Of course, supplying arguments at the command line can quickly become cumbersome. Saber also allows you to specify a configuration file, which can be specified like so (saber) $ python -m saber.train --config_filepath path/to/config.ini Copy the contents of the default config file to a new *.ini file in order to get started. Note that arguments supplied at the command line overwrite those found in the configuration file. For example (saber) $ python -m saber.train --dataset_folder path/to/dataset --k_folds 10 would overwrite the arguments for dataset_folder and k_folds found in the configuration file.","title":"Command line tool"},{"location":"guide_to_saber_api/#python-package","text":"You can also import Saber and interact with it as a python package. Saber exposes its functionality through the SequenceProcessor class. Here is just about everything Saber does in one script: from saber.sequence_processor import SequenceProcessor # First, create a SequenceProcessor object, which exposes Sabers functionality sp = SequenceProcessor () # Load a dataset and create a model (provide a list of datasets to use multi-task learning!) sp . load_dataset ( 'path/to/datasets/GENIA' ) sp . create_model () # Train and save a model sp . fit () sp . save ( 'pretrained_models/GENIA' ) # Load a model del sp sp = SequenceProcessor () sp . load ( 'pretrained_models/GENIA' ) # Perform prediction on raw text, get resulting annotation raw_text = 'The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53.' annotation = sp . annotate ( raw_text ) # Use transfer learning to continue training on a new dataset sp . load_dataset ( 'path/to/datasets/CRAFT' ) sp . fit ()","title":"Python package"},{"location":"guide_to_saber_api/#transfer-learning","text":"Transfer learning is as easy as training, saving, loading, and then continuing training of a model. Here is an example # Create and train a model on GENIA corpus sp = SequenceProcessor () sp . load_dataset ( 'path/to/datasets/GENIA' ) sp . create_model () sp . fit () sp . save ( 'pretrained_models/GENIA' ) # Load that model del sp sp = SequenceProcessor () sp . load ( 'pretrained_models/GENIA' ) # Use transfer learning to continue training on a new dataset sp . load_dataset ( 'path/to/datasets/CRAFT' ) sp . fit () Note that there is currently no way to easily do this with the command line interface, but I am working on it!","title":"Transfer learning"},{"location":"guide_to_saber_api/#multi-task-learning","text":"Multi-task learning is as easy as specifying multiple dataset paths, either in the config file, at the command line via the flag --dataset_folder , or as an argument to load_dataset() . The number of datasets is arbitrary. Here is an example using the last method sp = SequenceProcessor () # Simply pass multiple dataset paths as a list to load_dataset to use multi-task learning. sp . load_dataset ([ 'path/to/datasets/NCBI-Disease' , 'path/to/datasets/Linnaeus' ]) sp . create_model () sp . fit ()","title":"Multi-task learning"},{"location":"guide_to_saber_api/#saving-and-loading-models","text":"In the following sections we introduce the saving and loading of models.","title":"Saving and loading models"},{"location":"guide_to_saber_api/#saving-a-model","text":"Assuming the model has already been created (see above), we can easily save our model like so path_to_saved_model = 'path/to/pretrained_models/mymodel' sp . save ( path_to_saved_model )","title":"Saving a model"},{"location":"guide_to_saber_api/#loading-a-model","text":"Lets illustrate loading a model with a new SequenceProccesor object # Delete our previous SequenceProccesor object (if it exists) if 'sp' in locals (): del sp # Create a new SequenceProccesor object sp = SequenceProcessor () # Load a previous model sp . load ( path_to_saved_model )","title":"Loading a model"},{"location":"guide_to_saber_api/#juypter-notebooks","text":"First, grab the notebook. Go here , then press command / control + s , and save the notebook as lightning_tour.ipynb somewhere on your computer. Next, install JupyterLab by following the instructions here . Once installed, run: (saber) $ jupyter lab A new window will open in your browser. Use it to search for lightning_tour.ipynb on your computer. A couple of notes: If you activated a virtual environment, \" myenv \", make sure you see Python [venv:myenv] in the top right of the Jupyter notebook. If you are using conda, you need to run conda install nb_conda with your environment activated.","title":"Juypter notebooks"},{"location":"installation/","text":"Installation To install Saber, you will need python==3.6 . If not already installed, python==3.6 can be installed via: the official installer Homebrew , on MacOS ( brew install python3 ) Miniconda3 / Anaconda3 Use python --version at the command line to make sure installation was successful. Note: you may need to use python3 (not just python ) at the command line depending on your install method. (OPTIONAL) First, activate your virtual environment (see below for help): $ source activate saber ( saber ) $ Then, install Saber right from the repository with pip (saber) $ pip install git+https://github.com/BaderLab/saber.git or by cloning the repository and then using pip to install the package (saber) $ git clone https://github.com/BaderLab/saber.git (saber) $ cd saber (saber) $ pip install . You can also install Saber by cloning this repository and running python setup.py install from within the repository. Finally, you must also pip install the required Spacy model and the keras-contrib repositories # keras-contrib (saber) $ pip install git+https://www.github.com/keras-team/keras-contrib.git # NeuralCoref medium model built on top of Spacy, this might take a few minutes to download! (saber) $ pip install https://github.com/huggingface/neuralcoref-models/releases/download/en_coref_md-3.0.0/en_coref_md-3.0.0.tar.gz See Running tests for a way to verify your installation. (OPTIONAL) Creating and activating virtual environments When using pip it is generally recommended to install packages in a virtual environment to avoid modifying system state. To create a virtual environment named saber : Using virtualenv or venv Using virtualenv : $ virtualenv /path/to/new/venv/saber Using venv : $ python3 -m venv /path/to/new/venv/saber Next, you need to activate the environment. $ source /path/to/new/venv/saber/bin/activate # Notice your command prompt has changed to indicate that the environment is active ( saber ) $ Using Conda If you use Conda / Miniconda , you can create an environment named saber by running: $ conda create -n saber python = 3 .6 To activate the environment: $ source activate saber # Notice your command prompt has changed to indicate that the environment is active ( saber ) $ Note: you do not need to name the environment saber . Running tests Sabers test suite can be found in saber/tests . If Saber is already installed, you can run pytest on the installation directory: # Install pytest ( saber ) $ pip install pytest # Find out where Saber is installed ( saber ) $ INSTALL_DIR = $ ( python - c \"import os; import saber; print(os.path.dirname(saber.__file__))\" ) # Run tests on that installation directory ( saber ) $ python - m pytest $ INSTALL_DIR Alternatively, to clone Saber, install it, and run the test suite all in one go: (saber) $ git clone https://github.com/BaderLab/saber.git (saber) $ cd saber (saber) $ python setup.py test","title":"Installation"},{"location":"installation/#installation","text":"To install Saber, you will need python==3.6 . If not already installed, python==3.6 can be installed via: the official installer Homebrew , on MacOS ( brew install python3 ) Miniconda3 / Anaconda3 Use python --version at the command line to make sure installation was successful. Note: you may need to use python3 (not just python ) at the command line depending on your install method. (OPTIONAL) First, activate your virtual environment (see below for help): $ source activate saber ( saber ) $ Then, install Saber right from the repository with pip (saber) $ pip install git+https://github.com/BaderLab/saber.git or by cloning the repository and then using pip to install the package (saber) $ git clone https://github.com/BaderLab/saber.git (saber) $ cd saber (saber) $ pip install . You can also install Saber by cloning this repository and running python setup.py install from within the repository. Finally, you must also pip install the required Spacy model and the keras-contrib repositories # keras-contrib (saber) $ pip install git+https://www.github.com/keras-team/keras-contrib.git # NeuralCoref medium model built on top of Spacy, this might take a few minutes to download! (saber) $ pip install https://github.com/huggingface/neuralcoref-models/releases/download/en_coref_md-3.0.0/en_coref_md-3.0.0.tar.gz See Running tests for a way to verify your installation.","title":"Installation"},{"location":"installation/#optional-creating-and-activating-virtual-environments","text":"When using pip it is generally recommended to install packages in a virtual environment to avoid modifying system state. To create a virtual environment named saber :","title":"(OPTIONAL) Creating and activating virtual environments"},{"location":"installation/#using-virtualenv-or-venv","text":"Using virtualenv : $ virtualenv /path/to/new/venv/saber Using venv : $ python3 -m venv /path/to/new/venv/saber Next, you need to activate the environment. $ source /path/to/new/venv/saber/bin/activate # Notice your command prompt has changed to indicate that the environment is active ( saber ) $","title":"Using virtualenv or venv"},{"location":"installation/#using-conda","text":"If you use Conda / Miniconda , you can create an environment named saber by running: $ conda create -n saber python = 3 .6 To activate the environment: $ source activate saber # Notice your command prompt has changed to indicate that the environment is active ( saber ) $ Note: you do not need to name the environment saber .","title":"Using Conda"},{"location":"installation/#running-tests","text":"Sabers test suite can be found in saber/tests . If Saber is already installed, you can run pytest on the installation directory: # Install pytest ( saber ) $ pip install pytest # Find out where Saber is installed ( saber ) $ INSTALL_DIR = $ ( python - c \"import os; import saber; print(os.path.dirname(saber.__file__))\" ) # Run tests on that installation directory ( saber ) $ python - m pytest $ INSTALL_DIR Alternatively, to clone Saber, install it, and run the test suite all in one go: (saber) $ git clone https://github.com/BaderLab/saber.git (saber) $ cd saber (saber) $ python setup.py test","title":"Running tests"},{"location":"quick_start/","text":"Quick Start If your goal is simply to use Saber to annotate biomedical text, then you can either use the web-service or a pre-trained model . Web-service To use Saber as a local web-service, run: (saber) $ python -m saber.app or, if you prefer, you can pull & run the Saber image from Docker Hub : # Pull Saber image from Docker Hub $ docker pull pathwaycommons/saber # Run docker (use `-dt` instead of `-it` to run container in background) $ docker run -it --rm -p 5000 :5000 --name saber pathwaycommons/saber Alternatively, you can clone the GitHub repository and build the container from the Dockerfile with docker build -t saber . There are currently two endpoints, /annotate/text and /annotate/pmid . Both expect a POST request with a JSON payload, e.g. { \"text\" : \"The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53.\" } or { \"pmid\" : 11835401 } For example, running the web-service locally and using cURL : ( saber ) $ curl -X POST 'http://localhost:5000/annotate/text' \\ --data '{\"text\": ' The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53. '}' Documentation for the Saber web-service API can be found here . We hope to provide a live version of the web-service soon! Pre-trained models First, import SequenceProcessor . This class coordinates training, annotation, saving and loading of models and datasets. In short, this is the interface to Saber from saber.sequence_processor import SequenceProcessor To load a pre-trained model, we first create a SequenceProcessor object sp = SequenceProcessor () and then load the model of our choice sp . load ( 'PRGE' ) You can see all the pre-trained models in the web-service API docs or, alternatively, by running the following line of code from saber.constants import ENTITIES ; print ( list ( ENTITIES . keys ())) To annotate text with the model, just call the annotate() method sp . annotate ( \"The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53.\" ) Coreference Resolution Coreference occurs when two or more expressions in a text refer to the same person or thing, that is, they have the same referent . Take the following example: \"IL-6 supports tumour growth and metastasising in terminal patients, and it significantly engages in cancer cachexia (including anorexia) and depression associated with malignancy.\" Clearly, \"it\" referes to \"IL-6\" . If we do not resolve this coreference, then \"it\" will not be labeled as an entity and any relation or event it is mentioned in will not be extracted. Saber uses NeuralCoref , a state-of-the-art coreference resolution tool based on neural nets and built on top of Spacy . To use it, just supply the argument coref=True (which is False by default) to the annotate() method text = \"IL-6 supports tumour growth and metastasising in terminal patients, and it significantly engages in cancer cachexia (including anorexia) and depression associated with malignancy.\" # WITHOUT coreference resolution sp . annotate ( text , coref = False ) # WITH coreference resolution sp . annotate ( text , coref = True ) Note that if you are using the web-service, simply supply \"coref\": true in your JSON payload to resolve coreferences. Saber currently takes the simplest possible approach: replace all coreference mentions with their referent, and then feed the resolved text to the model that identifies named entities. Working with annotations The annotate() method returns a simple dict object ann = sp . annotate ( \"The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53.\" ) which contains the keys title , text and ents : title : contains the title of the article, if provided text : contains the text (which is minimally processed) the model was deployed on ents : contains a list of entities present in the text that were annotated by the model For example, to see all entities annotated by the model, call ann [ 'ents' ] Converting annotations to JSON The annotate() method returns a dict object, but can be converted to a JSON formatted string for ease-of-use in downstream applications import json # convert to json object json_ann = json . dumps ( ann ) # convert back to python dictionary ann = json . loads ( json_ann )","title":"Quick Start"},{"location":"quick_start/#quick-start","text":"If your goal is simply to use Saber to annotate biomedical text, then you can either use the web-service or a pre-trained model .","title":"Quick Start"},{"location":"quick_start/#web-service","text":"To use Saber as a local web-service, run: (saber) $ python -m saber.app or, if you prefer, you can pull & run the Saber image from Docker Hub : # Pull Saber image from Docker Hub $ docker pull pathwaycommons/saber # Run docker (use `-dt` instead of `-it` to run container in background) $ docker run -it --rm -p 5000 :5000 --name saber pathwaycommons/saber Alternatively, you can clone the GitHub repository and build the container from the Dockerfile with docker build -t saber . There are currently two endpoints, /annotate/text and /annotate/pmid . Both expect a POST request with a JSON payload, e.g. { \"text\" : \"The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53.\" } or { \"pmid\" : 11835401 } For example, running the web-service locally and using cURL : ( saber ) $ curl -X POST 'http://localhost:5000/annotate/text' \\ --data '{\"text\": ' The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53. '}' Documentation for the Saber web-service API can be found here . We hope to provide a live version of the web-service soon!","title":"Web-service"},{"location":"quick_start/#pre-trained-models","text":"First, import SequenceProcessor . This class coordinates training, annotation, saving and loading of models and datasets. In short, this is the interface to Saber from saber.sequence_processor import SequenceProcessor To load a pre-trained model, we first create a SequenceProcessor object sp = SequenceProcessor () and then load the model of our choice sp . load ( 'PRGE' ) You can see all the pre-trained models in the web-service API docs or, alternatively, by running the following line of code from saber.constants import ENTITIES ; print ( list ( ENTITIES . keys ())) To annotate text with the model, just call the annotate() method sp . annotate ( \"The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53.\" )","title":"Pre-trained models"},{"location":"quick_start/#coreference-resolution","text":"Coreference occurs when two or more expressions in a text refer to the same person or thing, that is, they have the same referent . Take the following example: \"IL-6 supports tumour growth and metastasising in terminal patients, and it significantly engages in cancer cachexia (including anorexia) and depression associated with malignancy.\" Clearly, \"it\" referes to \"IL-6\" . If we do not resolve this coreference, then \"it\" will not be labeled as an entity and any relation or event it is mentioned in will not be extracted. Saber uses NeuralCoref , a state-of-the-art coreference resolution tool based on neural nets and built on top of Spacy . To use it, just supply the argument coref=True (which is False by default) to the annotate() method text = \"IL-6 supports tumour growth and metastasising in terminal patients, and it significantly engages in cancer cachexia (including anorexia) and depression associated with malignancy.\" # WITHOUT coreference resolution sp . annotate ( text , coref = False ) # WITH coreference resolution sp . annotate ( text , coref = True ) Note that if you are using the web-service, simply supply \"coref\": true in your JSON payload to resolve coreferences. Saber currently takes the simplest possible approach: replace all coreference mentions with their referent, and then feed the resolved text to the model that identifies named entities.","title":"Coreference Resolution"},{"location":"quick_start/#working-with-annotations","text":"The annotate() method returns a simple dict object ann = sp . annotate ( \"The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53.\" ) which contains the keys title , text and ents : title : contains the title of the article, if provided text : contains the text (which is minimally processed) the model was deployed on ents : contains a list of entities present in the text that were annotated by the model For example, to see all entities annotated by the model, call ann [ 'ents' ]","title":"Working with annotations"},{"location":"quick_start/#converting-annotations-to-json","text":"The annotate() method returns a dict object, but can be converted to a JSON formatted string for ease-of-use in downstream applications import json # convert to json object json_ann = json . dumps ( ann ) # convert back to python dictionary ann = json . loads ( json_ann )","title":"Converting annotations to JSON"},{"location":"resources/","text":"Resources Saber is ready to go out-of-the box when using the web-service or a pre-trained model . However, if you plan on training you own models, you will need to provide a dataset (or datasets!) and, ideally, pre-trained word embeddings. Datasets Currently, Saber requires corpora to be in a CoNLL format with a BIO or IOBES tag scheme, e.g.: Selegiline B-CHED - O induced O postural B-DISO hypotension I-DISO ... Corpora in such a format are collected in here for convenience. Many of the corpora in the BIO and IOBES tag format were originally collected by Crichton et al ., 2017 , here . In this format, the first column contains each token of an input sentence, the last column contains the tokens tag, all columns are separated by tabs, and all sentences by a newline. Of course, not all corpora are distributed in the CoNLL format: Corpora in the Standoff format can be converted to CoNLL format using this tool. Corpora in PubTator format can be converted to Standoff first using this tool. Saber infers the \"training strategy\" based on the structure of the dataset folder: To use k-fold cross-validation, simply provide a train.* file in your dataset folder. E.g. . \u251c\u2500\u2500 NCBI-Disease \u2502 \u2514\u2500\u2500 train.tsv To use a train/valid/test strategy, provide train.* and test.* files in your dataset folder. Optionally, you can provide a valid.* file. If not provided, a random 10% of examples from train.* are used as the validation set. E.g. . \u251c\u2500\u2500 NCBI-Disease \u2502 \u251c\u2500\u2500 test.tsv \u2502 \u2514\u2500\u2500 train.tsv Word embeddings When training new models, you can (and should) provide your own pre-trained word embeddings with the pretrained_embeddings argument (either at the command line or in the configuration file). Saber expects all word embeddings to be in the word2vec file format. Pyysalo et al . 2013 provide word embeddings that work quite well in the biomedical domain, which can be downloaded here . Alternatively, from the command line call: # Replace this with a location you want to save the embeddings to $ mkdir path/to/word_embeddings # Note: this file is over 4GB $ wget http://evexdb.org/pmresources/vec-space-models/wikipedia-pubmed-and-PMC-w2v.bin -O path/to/word_embeddings GloVe To use GloVe embeddings, just convert them to the word2vec format first: ( saber ) $ python >>> from gensim.scripts.glove2word2vec import glove2word2vec >>> glove_input_file = 'glove.txt' >>> word2vec_output_file = 'word2vec.txt' >>> glove2word2vec ( glove_input_file , word2vec_output_file )","title":"Resources"},{"location":"resources/#resources","text":"Saber is ready to go out-of-the box when using the web-service or a pre-trained model . However, if you plan on training you own models, you will need to provide a dataset (or datasets!) and, ideally, pre-trained word embeddings.","title":"Resources"},{"location":"resources/#datasets","text":"Currently, Saber requires corpora to be in a CoNLL format with a BIO or IOBES tag scheme, e.g.: Selegiline B-CHED - O induced O postural B-DISO hypotension I-DISO ... Corpora in such a format are collected in here for convenience. Many of the corpora in the BIO and IOBES tag format were originally collected by Crichton et al ., 2017 , here . In this format, the first column contains each token of an input sentence, the last column contains the tokens tag, all columns are separated by tabs, and all sentences by a newline. Of course, not all corpora are distributed in the CoNLL format: Corpora in the Standoff format can be converted to CoNLL format using this tool. Corpora in PubTator format can be converted to Standoff first using this tool. Saber infers the \"training strategy\" based on the structure of the dataset folder: To use k-fold cross-validation, simply provide a train.* file in your dataset folder. E.g. . \u251c\u2500\u2500 NCBI-Disease \u2502 \u2514\u2500\u2500 train.tsv To use a train/valid/test strategy, provide train.* and test.* files in your dataset folder. Optionally, you can provide a valid.* file. If not provided, a random 10% of examples from train.* are used as the validation set. E.g. . \u251c\u2500\u2500 NCBI-Disease \u2502 \u251c\u2500\u2500 test.tsv \u2502 \u2514\u2500\u2500 train.tsv","title":"Datasets"},{"location":"resources/#word-embeddings","text":"When training new models, you can (and should) provide your own pre-trained word embeddings with the pretrained_embeddings argument (either at the command line or in the configuration file). Saber expects all word embeddings to be in the word2vec file format. Pyysalo et al . 2013 provide word embeddings that work quite well in the biomedical domain, which can be downloaded here . Alternatively, from the command line call: # Replace this with a location you want to save the embeddings to $ mkdir path/to/word_embeddings # Note: this file is over 4GB $ wget http://evexdb.org/pmresources/vec-space-models/wikipedia-pubmed-and-PMC-w2v.bin -O path/to/word_embeddings","title":"Word embeddings"},{"location":"resources/#glove","text":"To use GloVe embeddings, just convert them to the word2vec format first: ( saber ) $ python >>> from gensim.scripts.glove2word2vec import glove2word2vec >>> glove_input_file = 'glove.txt' >>> word2vec_output_file = 'word2vec.txt' >>> glove2word2vec ( glove_input_file , word2vec_output_file )","title":"GloVe"}]}