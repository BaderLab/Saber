{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Saber ( S equence A nnotator for B iomedical E ntities and R elations) is a deep-learning based tool for information extraction in the biomedical domain. The neural network model used is a BiLSTM-CRF [ 1 , 2 ]; a state-of-the-art architecture for sequence labelling. The model is implemented using Keras . Pull requests are welcome! If you encounter any bugs, please open an issue in the GitHub repository .","title":"Home"},{"location":"#home","text":"Saber ( S equence A nnotator for B iomedical E ntities and R elations) is a deep-learning based tool for information extraction in the biomedical domain. The neural network model used is a BiLSTM-CRF [ 1 , 2 ]; a state-of-the-art architecture for sequence labelling. The model is implemented using Keras . Pull requests are welcome! If you encounter any bugs, please open an issue in the GitHub repository .","title":"Home"},{"location":"guide_to_saber_api/","text":"Guide to the Saber API You can interact with Saber as a web-service (explained in Quick start ), command line tool, python package, or via the Juypter notebooks. If you created a virtual environment, remember to activate it first . Command line tool Currently, the command line tool simply trains the model. To use it, call ( saber ) $ python -m saber.train along with any command line arguments. For example, to train the model on the NCBI Disease corpus ( saber ) $ python -m saber.train --dataset_folder NCBI_disease_BIO See Resources for help preparing datasets for training. Run python -m saber.train -h to see all possible arguments. Of course, supplying arguments at the command line can quickly become cumbersome. Saber also allows you to specify a configuration file, which can be specified like so ( saber ) $ python -m saber.train --config_filepath path/to/config.ini Copy the contents of the default config file to a new *.ini file in order to get started. Note that arguments supplied at the command line overwrite those found in the configuration file. For example ( saber ) $ python -m saber.train --dataset_folder path/to/dataset --k_folds 10 would overwrite the arguments for dataset_folder and k_folds found in the configuration file. Python package You can also import Saber and interact with it as a python package. Saber exposes its functionality through the SequenceProcessor class. Here is just about everything Saber does in one script: from saber.sequence_processor import SequenceProcessor # First, create a SequenceProcessor object, which exposes Sabers functionality sp = SequenceProcessor () # Load a dataset and create a model (provide a list of datasets to use multi-task learning!) sp . load_dataset ( 'path/to/datasets/GENIA' ) sp . create_model () # Train and save a model sp . fit () sp . save ( 'pretrained_models/GENIA' ) # Load a model del sp sp = SequenceProcessor () sp . load ( 'pretrained_models/GENIA' ) # Perform prediction on raw text, get resulting annotation raw_text = 'The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53.' annotation = sp . annotate ( raw_text ) # Use transfer learning to continue training on a new dataset sp . load_dataset ( 'path/to/datasets/CRAFT' ) sp . fit () Juypter notebooks First, grab the notebook. Go here , then press command / control + s , and save the notebook as lightning_tour.ipynb somewhere on your computer. Next, install JupyterLab by following the instructions here . Once installed, run: ( saber ) $ jupyter lab A new window will open in your browser. Use it to search for lightning_tour.ipynb on your computer. A couple of notes: If you activated a virtual environment, \" myenv \", make sure you see Python [venv:myenv] in the top right of the Jupyter notebook. If you are using conda, you need to run conda install nb_conda with your environment activated.","title":"Guide to the Saber API"},{"location":"guide_to_saber_api/#guide-to-the-saber-api","text":"You can interact with Saber as a web-service (explained in Quick start ), command line tool, python package, or via the Juypter notebooks. If you created a virtual environment, remember to activate it first .","title":"Guide to the Saber API"},{"location":"guide_to_saber_api/#command-line-tool","text":"Currently, the command line tool simply trains the model. To use it, call ( saber ) $ python -m saber.train along with any command line arguments. For example, to train the model on the NCBI Disease corpus ( saber ) $ python -m saber.train --dataset_folder NCBI_disease_BIO See Resources for help preparing datasets for training. Run python -m saber.train -h to see all possible arguments. Of course, supplying arguments at the command line can quickly become cumbersome. Saber also allows you to specify a configuration file, which can be specified like so ( saber ) $ python -m saber.train --config_filepath path/to/config.ini Copy the contents of the default config file to a new *.ini file in order to get started. Note that arguments supplied at the command line overwrite those found in the configuration file. For example ( saber ) $ python -m saber.train --dataset_folder path/to/dataset --k_folds 10 would overwrite the arguments for dataset_folder and k_folds found in the configuration file.","title":"Command line tool"},{"location":"guide_to_saber_api/#python-package","text":"You can also import Saber and interact with it as a python package. Saber exposes its functionality through the SequenceProcessor class. Here is just about everything Saber does in one script: from saber.sequence_processor import SequenceProcessor # First, create a SequenceProcessor object, which exposes Sabers functionality sp = SequenceProcessor () # Load a dataset and create a model (provide a list of datasets to use multi-task learning!) sp . load_dataset ( 'path/to/datasets/GENIA' ) sp . create_model () # Train and save a model sp . fit () sp . save ( 'pretrained_models/GENIA' ) # Load a model del sp sp = SequenceProcessor () sp . load ( 'pretrained_models/GENIA' ) # Perform prediction on raw text, get resulting annotation raw_text = 'The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53.' annotation = sp . annotate ( raw_text ) # Use transfer learning to continue training on a new dataset sp . load_dataset ( 'path/to/datasets/CRAFT' ) sp . fit ()","title":"Python package"},{"location":"guide_to_saber_api/#juypter-notebooks","text":"First, grab the notebook. Go here , then press command / control + s , and save the notebook as lightning_tour.ipynb somewhere on your computer. Next, install JupyterLab by following the instructions here . Once installed, run: ( saber ) $ jupyter lab A new window will open in your browser. Use it to search for lightning_tour.ipynb on your computer. A couple of notes: If you activated a virtual environment, \" myenv \", make sure you see Python [venv:myenv] in the top right of the Jupyter notebook. If you are using conda, you need to run conda install nb_conda with your environment activated.","title":"Juypter notebooks"},{"location":"installation/","text":"Installation To install Saber, you will need python==3.6 . If not already installed, python==3.6 can be installed via: the official installer Homebrew , on MacOS ( brew install python3 ) Miniconda3 / Anaconda3 Use python --version at the command line to make sure installation was successful. Note: you may need to use python3 (not just python ) at the command line depending on your install method. (OPTIONAL) First, activate your virtual environment (see below for help): $ source activate saber ( saber ) $ Then, install Saber right from the repository with pip ( saber ) $ pip install git+https://github.com/BaderLab/saber.git or by cloning the repository and then using pip to install the package ( saber ) $ git clone https://github.com/BaderLab/saber.git ( saber ) $ cd saber ( saber ) $ pip install . You can also install Saber by cloning this repository and running python setup.py install from within the repository. Finally, you must also pip install the required Spacy model and the keras-contrib repositories # keras-contrib ( saber ) $ pip install git+https://www.github.com/keras-team/keras-contrib.git # spacy small english model ( saber ) $ pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#en_core_web_sm See Running tests for a way to verify your installation. (OPTIONAL) Creating and activating virtual environments When using pip it is generally recommended to install packages in a virtual environment to avoid modifying system state. To create a virtual environment named saber : Using virtualenv or venv Using virtualenv : $ virtualenv /path/to/new/venv/saber Using venv : $ python3 -m venv /path/to/new/venv/saber Next, you need to activate the environment. $ source /path/to/new/venv/saber/bin/activate # Notice your command prompt has changed to indicate that the environment is active ( saber ) $ Using Conda If you use Conda / Miniconda , you can create an environment named saber by running: $ conda create -n saber python = 3 .6 To activate the environment: $ source activate saber # Notice your command prompt has changed to indicate that the environment is active ( saber ) $ Note: you do not need to name the environment saber . Running tests Sabers test suite can be found in saber/tests . In order to run the tests, you'll usually want to clone the repository locally. Make sure to install all required development dependencies defined in requirements.txt . Additionally, you will need to install pytest : ( saber ) $ pip install pytest To run the tests: ( saber ) $ cd path/to/saber ( saber ) $ py.test saber Alternatively, you can find out where Saber is installed and run pytest on that directory: # Find out where Saber is installed ( saber ) $ python -c \"import os; import saber; print(os.path.dirname(saber.__file__))\" # Run tests on that installation directory ( saber ) $ python -m pytest <Saber-directory>","title":"Installation"},{"location":"installation/#installation","text":"To install Saber, you will need python==3.6 . If not already installed, python==3.6 can be installed via: the official installer Homebrew , on MacOS ( brew install python3 ) Miniconda3 / Anaconda3 Use python --version at the command line to make sure installation was successful. Note: you may need to use python3 (not just python ) at the command line depending on your install method. (OPTIONAL) First, activate your virtual environment (see below for help): $ source activate saber ( saber ) $ Then, install Saber right from the repository with pip ( saber ) $ pip install git+https://github.com/BaderLab/saber.git or by cloning the repository and then using pip to install the package ( saber ) $ git clone https://github.com/BaderLab/saber.git ( saber ) $ cd saber ( saber ) $ pip install . You can also install Saber by cloning this repository and running python setup.py install from within the repository. Finally, you must also pip install the required Spacy model and the keras-contrib repositories # keras-contrib ( saber ) $ pip install git+https://www.github.com/keras-team/keras-contrib.git # spacy small english model ( saber ) $ pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#en_core_web_sm See Running tests for a way to verify your installation.","title":"Installation"},{"location":"installation/#optional-creating-and-activating-virtual-environments","text":"When using pip it is generally recommended to install packages in a virtual environment to avoid modifying system state. To create a virtual environment named saber :","title":"(OPTIONAL) Creating and activating virtual environments"},{"location":"installation/#using-virtualenv-or-venv","text":"Using virtualenv : $ virtualenv /path/to/new/venv/saber Using venv : $ python3 -m venv /path/to/new/venv/saber Next, you need to activate the environment. $ source /path/to/new/venv/saber/bin/activate # Notice your command prompt has changed to indicate that the environment is active ( saber ) $","title":"Using virtualenv or venv"},{"location":"installation/#using-conda","text":"If you use Conda / Miniconda , you can create an environment named saber by running: $ conda create -n saber python = 3 .6 To activate the environment: $ source activate saber # Notice your command prompt has changed to indicate that the environment is active ( saber ) $ Note: you do not need to name the environment saber .","title":"Using Conda"},{"location":"installation/#running-tests","text":"Sabers test suite can be found in saber/tests . In order to run the tests, you'll usually want to clone the repository locally. Make sure to install all required development dependencies defined in requirements.txt . Additionally, you will need to install pytest : ( saber ) $ pip install pytest To run the tests: ( saber ) $ cd path/to/saber ( saber ) $ py.test saber Alternatively, you can find out where Saber is installed and run pytest on that directory: # Find out where Saber is installed ( saber ) $ python -c \"import os; import saber; print(os.path.dirname(saber.__file__))\" # Run tests on that installation directory ( saber ) $ python -m pytest <Saber-directory>","title":"Running tests"},{"location":"quick_start/","text":"Quick Start If your goal is simply to use Saber to annotate biomedical text, then you can either use the web-service or a pre-trained model . Web-service To use Saber as a local web-service, run: ( saber ) $ python -m saber.app or to build & run Saber with Docker : # Build docker ( saber ) $ docker build -t saber . # Run docker (use `-it` instead of `-dt` to try it interactively) ( saber ) $ docker run --rm -p 5000 :5000 --name saber1 -dt saber There are currently two endpoints, /annotate/text and /annotate/pmid . Both expect a POST request with a JSON payload, e.g. { \"text\" : \"The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53.\" } or { \"pmid\" : 11835401 } For example, running the web-service locally and using cURL : ( saber ) $ curl -X POST 'http://localhost:5000/annotate/text' \\ --data '{\"text\": \"The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53.\"}' Documentation for the Saber web-service API can be found here . We hope to provide a live version of the web-service soon! Pre-trained models First, import SequenceProcessor . This class coordinates training, annotation, saving and loading of models and datasets. In short, this is the interface to Saber. from saber.sequence_processor import SequenceProcessor To load a pre-trained model, we first create a SequenceProcessor object sp = SequenceProcessor () and then load the model of our choice sp . load ( 'PRGE' ) You can see all the pre-trained models in the web-service API docs or, alternatively, by running the following line of code from saber.constants import ENTITIES ; print ( list ( ENTITIES . keys ())) To annotate text with the model, just call the annotate() method sp . annotate ( 'A Sos-1-E3b1 complex directs Rac activation by entering into a tricomplex with Eps8.' ) Working with annotations The annotate() method returns a simple dict object ann = sp . annotate ( 'A Sos-1-E3b1 complex directs Rac activation by entering into a tricomplex with Eps8.' ) which contains the keys title , text and ents : title : contains the title of the article, if provided text : contains the text (which is minimally processed) the model was deployed on ents : contains a list of entities present in the text that were annotated by the model For example, to see all entities annotated by the model, call ann [ 'ents' ] Converting annotations to JSON The annotate() method returns a dict object, but can be converted to a JSON formatted string for ease-of-use in downstream applications import json # convert to json object json_annotation = json . dumps ( annotation ) # convert back to python dictionary annotation = json . loads ( json_annotation )","title":"Quick start"},{"location":"quick_start/#quick-start","text":"If your goal is simply to use Saber to annotate biomedical text, then you can either use the web-service or a pre-trained model .","title":"Quick Start"},{"location":"quick_start/#web-service","text":"To use Saber as a local web-service, run: ( saber ) $ python -m saber.app or to build & run Saber with Docker : # Build docker ( saber ) $ docker build -t saber . # Run docker (use `-it` instead of `-dt` to try it interactively) ( saber ) $ docker run --rm -p 5000 :5000 --name saber1 -dt saber There are currently two endpoints, /annotate/text and /annotate/pmid . Both expect a POST request with a JSON payload, e.g. { \"text\" : \"The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53.\" } or { \"pmid\" : 11835401 } For example, running the web-service locally and using cURL : ( saber ) $ curl -X POST 'http://localhost:5000/annotate/text' \\ --data '{\"text\": \"The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53.\"}' Documentation for the Saber web-service API can be found here . We hope to provide a live version of the web-service soon!","title":"Web-service"},{"location":"quick_start/#pre-trained-models","text":"First, import SequenceProcessor . This class coordinates training, annotation, saving and loading of models and datasets. In short, this is the interface to Saber. from saber.sequence_processor import SequenceProcessor To load a pre-trained model, we first create a SequenceProcessor object sp = SequenceProcessor () and then load the model of our choice sp . load ( 'PRGE' ) You can see all the pre-trained models in the web-service API docs or, alternatively, by running the following line of code from saber.constants import ENTITIES ; print ( list ( ENTITIES . keys ())) To annotate text with the model, just call the annotate() method sp . annotate ( 'A Sos-1-E3b1 complex directs Rac activation by entering into a tricomplex with Eps8.' )","title":"Pre-trained models"},{"location":"quick_start/#working-with-annotations","text":"The annotate() method returns a simple dict object ann = sp . annotate ( 'A Sos-1-E3b1 complex directs Rac activation by entering into a tricomplex with Eps8.' ) which contains the keys title , text and ents : title : contains the title of the article, if provided text : contains the text (which is minimally processed) the model was deployed on ents : contains a list of entities present in the text that were annotated by the model For example, to see all entities annotated by the model, call ann [ 'ents' ]","title":"Working with annotations"},{"location":"quick_start/#converting-annotations-to-json","text":"The annotate() method returns a dict object, but can be converted to a JSON formatted string for ease-of-use in downstream applications import json # convert to json object json_annotation = json . dumps ( annotation ) # convert back to python dictionary annotation = json . loads ( json_annotation )","title":"Converting annotations to JSON"},{"location":"resources/","text":"Resources Saber is ready to go out-of-the box when using the web-service or a pre-trained model . However, if you plan on training you own models, you will need to provide a dataset (or datasets!) and, ideally, pre-trained word embeddings. Datasets Currently, Saber requires corpora to be in a CoNLL format with a BIO or IOBES tag scheme, e.g.: Selegiline B-CHED - O induced O postural B-DISO hypotension I-DISO ... Corpora in such a format are collected in here for convenience. Many of the corpora in the BIO and IOBES tag format were originally collected by Crichton et al ., 2017 , here . In this format, the first column contains each token of an input sentence, the last column contains the tokens tag, all columns are separated by tabs, and all sentences by a newline. Of course, not all corpora are distributed in the CoNLL format: Corpora in the Standoff format can be converted to CoNLL format using this tool. Corpora in PubTator format can be converted to Standoff first using this tool. Saber infers the \"training strategy\" based on the structure of the dataset folder: To use k-fold cross-validation, simply provide a train.* file in your dataset folder. E.g. . \u251c\u2500\u2500 NCBI-Disease \u2502 \u2514\u2500\u2500 train.tsv To use a train/valid/test strategy, provide train.* and test.* files in your dataset folder. Optionally, you can provide a valid.* file. If not provided, a random 10% of examples from train.* are used as the validation set. E.g. . \u251c\u2500\u2500 NCBI-Disease \u2502 \u251c\u2500\u2500 test.tsv \u2502 \u2514\u2500\u2500 train.tsv Word embeddings When training new models, you can (and should) provide your own pre-trained word embeddings with the pretrained_embeddings argument (either at the command line or in the configuration file). Saber expects all word embeddings to be in the word2vec file format. Pyysalo et al . 2013 provide word embeddings that work quite well in the biomedical domain, which can be downloaded here . Alternatively, from the command line call: # Replace this with a location you want to save the embeddings to $ mkdir path/to/word_embeddings # Note: this file is over 4GB $ wget http://evexdb.org/pmresources/vec-space-models/wikipedia-pubmed-and-PMC-w2v.bin -O path/to/word_embeddings GloVe To use GloVe embeddings, just convert them to the word2vec format first: ( saber ) $ python >>> from gensim.scripts.glove2word2vec import glove2word2vec >>> glove_input_file = 'glove.txt' >>> word2vec_output_file = 'word2vec.txt' >>> glove2word2vec ( glove_input_file, word2vec_output_file )","title":"Resources"},{"location":"resources/#resources","text":"Saber is ready to go out-of-the box when using the web-service or a pre-trained model . However, if you plan on training you own models, you will need to provide a dataset (or datasets!) and, ideally, pre-trained word embeddings.","title":"Resources"},{"location":"resources/#datasets","text":"Currently, Saber requires corpora to be in a CoNLL format with a BIO or IOBES tag scheme, e.g.: Selegiline B-CHED - O induced O postural B-DISO hypotension I-DISO ... Corpora in such a format are collected in here for convenience. Many of the corpora in the BIO and IOBES tag format were originally collected by Crichton et al ., 2017 , here . In this format, the first column contains each token of an input sentence, the last column contains the tokens tag, all columns are separated by tabs, and all sentences by a newline. Of course, not all corpora are distributed in the CoNLL format: Corpora in the Standoff format can be converted to CoNLL format using this tool. Corpora in PubTator format can be converted to Standoff first using this tool. Saber infers the \"training strategy\" based on the structure of the dataset folder: To use k-fold cross-validation, simply provide a train.* file in your dataset folder. E.g. . \u251c\u2500\u2500 NCBI-Disease \u2502 \u2514\u2500\u2500 train.tsv To use a train/valid/test strategy, provide train.* and test.* files in your dataset folder. Optionally, you can provide a valid.* file. If not provided, a random 10% of examples from train.* are used as the validation set. E.g. . \u251c\u2500\u2500 NCBI-Disease \u2502 \u251c\u2500\u2500 test.tsv \u2502 \u2514\u2500\u2500 train.tsv","title":"Datasets"},{"location":"resources/#word-embeddings","text":"When training new models, you can (and should) provide your own pre-trained word embeddings with the pretrained_embeddings argument (either at the command line or in the configuration file). Saber expects all word embeddings to be in the word2vec file format. Pyysalo et al . 2013 provide word embeddings that work quite well in the biomedical domain, which can be downloaded here . Alternatively, from the command line call: # Replace this with a location you want to save the embeddings to $ mkdir path/to/word_embeddings # Note: this file is over 4GB $ wget http://evexdb.org/pmresources/vec-space-models/wikipedia-pubmed-and-PMC-w2v.bin -O path/to/word_embeddings","title":"Word embeddings"},{"location":"resources/#glove","text":"To use GloVe embeddings, just convert them to the word2vec format first: ( saber ) $ python >>> from gensim.scripts.glove2word2vec import glove2word2vec >>> glove_input_file = 'glove.txt' >>> word2vec_output_file = 'word2vec.txt' >>> glove2word2vec ( glove_input_file, word2vec_output_file )","title":"GloVe"}]}