{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Saber ( S equence A nnotator for B iomedical E ntities and R elations) is a deep-learning based tool for information extraction in the biomedical domain. The neural network model used is a BiLSTM-CRF [ 1 , 2 ]; a state-of-the-art architecture for sequence labelling. The model is implemented using Keras / Tensorflow . The goal is that Saber will eventually perform all the important steps in text-mining of biomedical literature: Coreference resolution ( ) Biomedical named entity recognition (BioNER) ( ) Entity linking / grounding / normalization ( ) Simple relation extraction ( ) Event extraction ( ) Pull requests are welcome! If you encounter any bugs, please open an issue in the GitHub repository .","title":"About"},{"location":"guide_to_saber_api/","text":"Guide to the Saber API You can interact with Saber as a web-service (explained in Quick Start: Web-service ), command line tool , or as a python package . If you created a virtual environment, remember to activate it first . Command line tool Currently, the command line tool simply trains the model. To use it, call (saber) $ python -m saber.cli.train along with any command line arguments. For example, to train the model on the NCBI Disease corpus (saber) $ python -m saber.cli.train --dataset_folder NCBI_Disease_BIO Tip See Resources: Datasets for help preparing datasets and word embeddings for training. Run python -m saber.cli.train --help to see all possible arguments. Of course, supplying arguments at the command line can quickly become cumbersome. Saber also allows you to provide a configuration file, which can be specified like so (saber) $ python -m saber.cli.train --config_filepath path/to/config.ini Copy the contents of the default config file to a new *.ini file in order to get started. Note Arguments supplied at the command line overwrite those found in the configuration file, e.g., (saber) $ python -m saber.cli.train --dataset_folder path/to/dataset --k_folds 10 would overwrite the arguments for dataset_folder and k_folds found in the configuration file. Python package You can also import Saber and interact with it as a python package. Saber exposes its functionality through the Saber class. Here is just about everything Saber does in one script: from saber.saber import Saber # First, create a Saber object, which exposes Sabers functionality saber = Saber () # Load a dataset and create a model (provide a list of datasets to use multi-task learning!) saber . load_dataset ( path/to/datasets/GENIA ) saber . build ( model_name = MT-LSTM-CRF ) # Train and save a model saber . train () saber . save ( pretrained_models/GENIA ) # Load a model del saber saber = Saber () saber . load ( pretrained_models/GENIA ) # Perform prediction on raw text, get resulting annotation raw_text = The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53. annotation = saber . annotate ( raw_text ) # Use transfer learning to continue training on a new dataset saber . load_dataset ( path/to/datasets/CRAFT ) saber . train () Transfer learning Transfer learning is as easy as training, saving, loading, and then continuing training of a model. Here is an example # Create and train a model on GENIA corpus saber = Saber () saber . load_dataset ( path/to/datasets/GENIA ) saber . build ( model_name = MT-LSTM-CRF ) saber . train () saber . save ( pretrained_models/GENIA ) # Load that model del saber saber = Saber () saber . load ( pretrained_models/GENIA ) # Use transfer learning to continue training on a new dataset saber . load_dataset ( path/to/datasets/CRAFT ) saber . train () Note This is currently only supported by the mt-lstm-crf model. Multi-task learning Multi-task learning is as easy as specifying multiple dataset paths, either in the config file, at the command line via the flag --dataset_folder , or as an argument to load_dataset() . The number of datasets is arbitrary. Here is an example using the last method saber = Saber () # Simply pass multiple dataset paths as a list to load_dataset to use multi-task learning. saber . load_dataset ([ path/to/datasets/NCBI_Disease , path/to/datasets/Linnaeus ]) saber . build ( model_name = MT-LSTM-CRF ) saber . train () Note This is currently only supported by the mt-lstm-crf model. Training on GPUs Saber will automatically train on as many GPUs as are available. In order for this to work, you must have CUDA and, optionally, CudDNN installed. If you are using conda to manage your environment, then these are installed for you when you call (saber) $ conda install tensorflow-gpu Otherwise, install them yourself and use pip to install tensorflow-gpu (saber) $ pip install tensorflow-gpu To control which GPUs Saber trains on, you can use the CUDA_VISIBLE_DEVICES environment variable, e.g., # To train exclusively on CPU (saber) $ CUDA_VISIBLE_DEVICES= python -m saber.cli.train # To train on 1 GPU with ID=0 (saber) $ CUDA_VISIBLE_DEVICES= 0 python -m saber.cli.train # To train on 2 GPUs with IDs=0,2 (saber) $ CUDA_VISIBLE_DEVICES= 0,2 python -m saber.cli.train Tip You can get information about your NVIDIA GPUs by typing nvidia-smi at the command line (assuming the GPUs are setup properly and the nvidia driver is installed). Saving and loading models In the following sections we introduce the saving and loading of models. Saving a model Assuming the model has already been created (see above), we can easily save our model like so save_dir = path/to/pretrained_models/mymodel saber . save ( save_dir ) Loading a model Lets illustrate loading a model with a new Saber object # Delete our previous Saber object (if it exists) del saber # Create a new Saber object saber = Saber () # Load a previous model saber . load ( path_to_saved_model )","title":"Guide to the Saber API"},{"location":"guide_to_saber_api/#guide-to-the-saber-api","text":"You can interact with Saber as a web-service (explained in Quick Start: Web-service ), command line tool , or as a python package . If you created a virtual environment, remember to activate it first .","title":"Guide to the Saber API"},{"location":"guide_to_saber_api/#command-line-tool","text":"Currently, the command line tool simply trains the model. To use it, call (saber) $ python -m saber.cli.train along with any command line arguments. For example, to train the model on the NCBI Disease corpus (saber) $ python -m saber.cli.train --dataset_folder NCBI_Disease_BIO Tip See Resources: Datasets for help preparing datasets and word embeddings for training. Run python -m saber.cli.train --help to see all possible arguments. Of course, supplying arguments at the command line can quickly become cumbersome. Saber also allows you to provide a configuration file, which can be specified like so (saber) $ python -m saber.cli.train --config_filepath path/to/config.ini Copy the contents of the default config file to a new *.ini file in order to get started. Note Arguments supplied at the command line overwrite those found in the configuration file, e.g., (saber) $ python -m saber.cli.train --dataset_folder path/to/dataset --k_folds 10 would overwrite the arguments for dataset_folder and k_folds found in the configuration file.","title":"Command line tool"},{"location":"guide_to_saber_api/#python-package","text":"You can also import Saber and interact with it as a python package. Saber exposes its functionality through the Saber class. Here is just about everything Saber does in one script: from saber.saber import Saber # First, create a Saber object, which exposes Sabers functionality saber = Saber () # Load a dataset and create a model (provide a list of datasets to use multi-task learning!) saber . load_dataset ( path/to/datasets/GENIA ) saber . build ( model_name = MT-LSTM-CRF ) # Train and save a model saber . train () saber . save ( pretrained_models/GENIA ) # Load a model del saber saber = Saber () saber . load ( pretrained_models/GENIA ) # Perform prediction on raw text, get resulting annotation raw_text = The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53. annotation = saber . annotate ( raw_text ) # Use transfer learning to continue training on a new dataset saber . load_dataset ( path/to/datasets/CRAFT ) saber . train ()","title":"Python package"},{"location":"guide_to_saber_api/#transfer-learning","text":"Transfer learning is as easy as training, saving, loading, and then continuing training of a model. Here is an example # Create and train a model on GENIA corpus saber = Saber () saber . load_dataset ( path/to/datasets/GENIA ) saber . build ( model_name = MT-LSTM-CRF ) saber . train () saber . save ( pretrained_models/GENIA ) # Load that model del saber saber = Saber () saber . load ( pretrained_models/GENIA ) # Use transfer learning to continue training on a new dataset saber . load_dataset ( path/to/datasets/CRAFT ) saber . train () Note This is currently only supported by the mt-lstm-crf model.","title":"Transfer learning"},{"location":"guide_to_saber_api/#multi-task-learning","text":"Multi-task learning is as easy as specifying multiple dataset paths, either in the config file, at the command line via the flag --dataset_folder , or as an argument to load_dataset() . The number of datasets is arbitrary. Here is an example using the last method saber = Saber () # Simply pass multiple dataset paths as a list to load_dataset to use multi-task learning. saber . load_dataset ([ path/to/datasets/NCBI_Disease , path/to/datasets/Linnaeus ]) saber . build ( model_name = MT-LSTM-CRF ) saber . train () Note This is currently only supported by the mt-lstm-crf model.","title":"Multi-task learning"},{"location":"guide_to_saber_api/#training-on-gpus","text":"Saber will automatically train on as many GPUs as are available. In order for this to work, you must have CUDA and, optionally, CudDNN installed. If you are using conda to manage your environment, then these are installed for you when you call (saber) $ conda install tensorflow-gpu Otherwise, install them yourself and use pip to install tensorflow-gpu (saber) $ pip install tensorflow-gpu To control which GPUs Saber trains on, you can use the CUDA_VISIBLE_DEVICES environment variable, e.g., # To train exclusively on CPU (saber) $ CUDA_VISIBLE_DEVICES= python -m saber.cli.train # To train on 1 GPU with ID=0 (saber) $ CUDA_VISIBLE_DEVICES= 0 python -m saber.cli.train # To train on 2 GPUs with IDs=0,2 (saber) $ CUDA_VISIBLE_DEVICES= 0,2 python -m saber.cli.train Tip You can get information about your NVIDIA GPUs by typing nvidia-smi at the command line (assuming the GPUs are setup properly and the nvidia driver is installed).","title":"Training on GPUs"},{"location":"guide_to_saber_api/#saving-and-loading-models","text":"In the following sections we introduce the saving and loading of models.","title":"Saving and loading models"},{"location":"guide_to_saber_api/#saving-a-model","text":"Assuming the model has already been created (see above), we can easily save our model like so save_dir = path/to/pretrained_models/mymodel saber . save ( save_dir )","title":"Saving a model"},{"location":"guide_to_saber_api/#loading-a-model","text":"Lets illustrate loading a model with a new Saber object # Delete our previous Saber object (if it exists) del saber # Create a new Saber object saber = Saber () # Load a previous model saber . load ( path_to_saved_model )","title":"Loading a model"},{"location":"installation/","text":"Installation To install Saber, you will need python3.6 . If not already installed, python3 can be installed via The official installer Homebrew , on MacOS ( brew install python3 ) Miniconda3 / Anaconda3 Note Run python --version at the command line to make sure installation was successful. You may need to type python3 (not just python ) depending on your install method. (OPTIONAL) Activate your virtual environment (see below for help) $ conda activate saber # Notice your command prompt has changed to indicate that the environment is active ( saber ) $ Latest PyPI stable release ( saber ) $ pip install saber Error The install from PyPI is currently broken, please install using the instructions below. Latest development release on GitHub Pull and install straight from GitHub ( saber ) $ pip install git+https://github.com/BaderLab/saber.git or install by cloning the repository ( saber ) $ git clone https://github.com/BaderLab/saber.git ( saber ) $ cd saber and then using either pip ( saber ) $ pip install -e . or setuptools ( saber ) $ python setup.py install Note See Running tests for a way to verify your installation. (OPTIONAL) Creating and activating virtual environments When using pip it is generally recommended to install packages in a virtual environment to avoid modifying system state. To create a virtual environment named saber Using virtualenv or venv Using virtualenv $ virtualenv --python = python3 /path/to/new/venv/saber Using venv $ python3 -m venv /path/to/new/venv/saber Next, you need to activate the environment. $ source /path/to/new/venv/saber/bin/activate # Notice your command prompt has changed to indicate that the environment is active ( saber ) $ Using Conda If you use Conda / Miniconda , you can create an environment named saber by running $ conda create -n saber python = 3 .6 To activate the environment $ conda activate saber # Notice your command prompt has changed to indicate that the environment is active ( saber ) $ Note You do not need to name the environment saber . Running tests Sabers test suite can be found in saber/tests . If Saber is already installed, you can run pytest on the installation directory # Install pytest ( saber ) $ pip install pytest # Find out where Saber is installed ( saber ) $ INSTALL_DIR = $ ( python - c import os; import saber; print(os.path.dirname(saber.__file__)) ) # Run tests on that installation directory ( saber ) $ python - m pytest $ INSTALL_DIR Alternatively, to clone Saber, install it, and run the test suite all in one go (saber) $ git clone https://github.com/BaderLab/saber.git (saber) $ cd saber (saber) $ python setup.py test","title":"Installation"},{"location":"installation/#installation","text":"To install Saber, you will need python3.6 . If not already installed, python3 can be installed via The official installer Homebrew , on MacOS ( brew install python3 ) Miniconda3 / Anaconda3 Note Run python --version at the command line to make sure installation was successful. You may need to type python3 (not just python ) depending on your install method. (OPTIONAL) Activate your virtual environment (see below for help) $ conda activate saber # Notice your command prompt has changed to indicate that the environment is active ( saber ) $","title":"Installation"},{"location":"installation/#latest-pypi-stable-release","text":"( saber ) $ pip install saber Error The install from PyPI is currently broken, please install using the instructions below.","title":"Latest PyPI stable release"},{"location":"installation/#latest-development-release-on-github","text":"Pull and install straight from GitHub ( saber ) $ pip install git+https://github.com/BaderLab/saber.git or install by cloning the repository ( saber ) $ git clone https://github.com/BaderLab/saber.git ( saber ) $ cd saber and then using either pip ( saber ) $ pip install -e . or setuptools ( saber ) $ python setup.py install Note See Running tests for a way to verify your installation.","title":"Latest development release on GitHub"},{"location":"installation/#optional-creating-and-activating-virtual-environments","text":"When using pip it is generally recommended to install packages in a virtual environment to avoid modifying system state. To create a virtual environment named saber","title":"(OPTIONAL) Creating and activating virtual environments"},{"location":"installation/#using-virtualenv-or-venv","text":"Using virtualenv $ virtualenv --python = python3 /path/to/new/venv/saber Using venv $ python3 -m venv /path/to/new/venv/saber Next, you need to activate the environment. $ source /path/to/new/venv/saber/bin/activate # Notice your command prompt has changed to indicate that the environment is active ( saber ) $","title":"Using virtualenv or venv"},{"location":"installation/#using-conda","text":"If you use Conda / Miniconda , you can create an environment named saber by running $ conda create -n saber python = 3 .6 To activate the environment $ conda activate saber # Notice your command prompt has changed to indicate that the environment is active ( saber ) $ Note You do not need to name the environment saber .","title":"Using Conda"},{"location":"installation/#running-tests","text":"Sabers test suite can be found in saber/tests . If Saber is already installed, you can run pytest on the installation directory # Install pytest ( saber ) $ pip install pytest # Find out where Saber is installed ( saber ) $ INSTALL_DIR = $ ( python - c import os; import saber; print(os.path.dirname(saber.__file__)) ) # Run tests on that installation directory ( saber ) $ python - m pytest $ INSTALL_DIR Alternatively, to clone Saber, install it, and run the test suite all in one go (saber) $ git clone https://github.com/BaderLab/saber.git (saber) $ cd saber (saber) $ python setup.py test","title":"Running tests"},{"location":"quick_start/","text":"Quick Start If your goal is to use Saber to annotate biomedical text, then you can either use the web-service or a pre-trained model . If you simply want to check Saber out, without installing anything locally, try the Google Colaboratory notebook. Google Colaboratory The fastest way to check out Saber is by following along with the Google Colaboratory notebook ( ). In order to be able to run the cells, select \"Open in Playground\" or, alternatively, save a copy to your own Google Drive account (File Save a copy in Drive). Web-service To use Saber as a local web-service, run (saber) $ python -m saber.cli.app or, if you prefer, you can pull run the Saber image from Docker Hub # Pull Saber image from Docker Hub $ docker pull pathwaycommons/saber # Run docker (use `-dt` instead of `-it` to run container in background) $ docker run -it --rm -p 5000:5000 --name saber pathwaycommons/saber Tip Alternatively, you can clone the GitHub repository and build the container from the Dockerfile with docker build -t saber . There are currently two endpoints, /annotate/text and /annotate/pmid . Both expect a POST request with a JSON payload, e.g. { text : The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53. } or { pmid : 11835401 } For example, with the web-service running locally Bash curl -X POST http://localhost:5000/annotate/text \\ --data { text : The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53. } python import requests # assuming you have requests package installed! url = http://localhost:5000/annotate/pmid payload = { text : The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53. } response = requests . post ( url , json = payload ) print ( response . text ) print ( response . status_code , response . reason ) Warning The first request to the web-service will be slow (~60s). This is because a large language model needs to be loaded into memory. Documentation for the Saber web-service API can be found here . We hope to provide a live version of the web-service soon! Pre-trained models First, import Saber . This class coordinates training, annotation, saving and loading of models and datasets. In short, this is the interface to Saber. from saber.saber import Saber To load a pre-trained model, first create a Saber object saber = Saber () and then load the model of our choice saber . load ( PRGE ) Tip See Resources: Pre-trained models for pre-trained model names and details. You will need an internet connection to download a pre-trained model. To annotate text with the model, just call the Saber.annotate() method saber . annotate ( The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53. ) Warning The Saber.annotate() method will be slow the first time you call it (~60s). This is because a large language model needs to be loaded into memory. Coreference Resolution Coreference occurs when two or more expressions in a text refer to the same person or thing, that is, they have the same referent . Take the following example: \" IL-6 supports tumour growth and metastasising in terminal patients, and it significantly engages in cancer cachexia (including anorexia) and depression associated with malignancy.\" Clearly, \" it \" referes to \" IL-6 \". If we do not resolve this coreference, then \" it \" will not be labeled as an entity and any relation or event it is mentioned in will not be extracted. Saber uses NeuralCoref , a state-of-the-art coreference resolution tool based on neural nets and built on top of Spacy . To use it, just supply the argument coref=True (which is False by default) to the Saber.annotate() method text = IL-6 supports tumour growth and metastasising in terminal patients, and it significantly engages in cancer cachexia (including anorexia) and depression associated with malignancy. # WITHOUT coreference resolution saber . annotate ( text , coref = False ) # WITH coreference resolution saber . annotate ( text , coref = True ) Note If you are using the web-service, simply supply \"coref\": true in your JSON payload to resolve coreferences. Saber currently takes the simplest possible approach: replace all coreference mentions with their referent, and then feed the resolved text to the model that identifies named entities. Grounding Grounding (sometimes called entity linking or normalization ) involves mapping each annotated entity to a unique identifier in an external resource such as a database or ontology. To ground entities in a call to Saber.annotate() , simply pass the argument ground=True saber . annotate ( The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53. , ground = True ) The grounding functionality is implemented by the EXTRACT 2.0 API . Note that you will need an internet connection or grounding will fail. Also note that Saber.annotate() will take slightly longer to return a response when ground=True (up to a few seconds). See Resources: Pre-trained models for a list of the the external resources each entity type (annotated by the pre-trained models) is grounded to. Note If you are using the web-service, simply supply \"ground\": true in your JSON payload to ground entities. Working with annotations The Saber.annotate() method returns a simple dict object ann = saber . annotate ( The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53. ) which contains the keys title , text and ents title : contains the title of the article, if provided text : contains the text (which is minimally processed) the model was deployed on ents : contains a list of entities present in the text that were annotated by the model For example, to see all entities annotated by the model, call ann [ ents ] Converting annotations to JSON The Saber.annotate() method returns a dict object, but can be converted to a JSON formatted string for ease-of-use in downstream applications import json # convert to json object json_ann = json . dumps ( ann ) # convert back to python dictionary ann = json . loads ( json_ann )","title":"Quick Start"},{"location":"quick_start/#quick-start","text":"If your goal is to use Saber to annotate biomedical text, then you can either use the web-service or a pre-trained model . If you simply want to check Saber out, without installing anything locally, try the Google Colaboratory notebook.","title":"Quick Start"},{"location":"quick_start/#google-colaboratory","text":"The fastest way to check out Saber is by following along with the Google Colaboratory notebook ( ). In order to be able to run the cells, select \"Open in Playground\" or, alternatively, save a copy to your own Google Drive account (File Save a copy in Drive).","title":"Google Colaboratory"},{"location":"quick_start/#web-service","text":"To use Saber as a local web-service, run (saber) $ python -m saber.cli.app or, if you prefer, you can pull run the Saber image from Docker Hub # Pull Saber image from Docker Hub $ docker pull pathwaycommons/saber # Run docker (use `-dt` instead of `-it` to run container in background) $ docker run -it --rm -p 5000:5000 --name saber pathwaycommons/saber Tip Alternatively, you can clone the GitHub repository and build the container from the Dockerfile with docker build -t saber . There are currently two endpoints, /annotate/text and /annotate/pmid . Both expect a POST request with a JSON payload, e.g. { text : The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53. } or { pmid : 11835401 } For example, with the web-service running locally Bash curl -X POST http://localhost:5000/annotate/text \\ --data { text : The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53. } python import requests # assuming you have requests package installed! url = http://localhost:5000/annotate/pmid payload = { text : The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53. } response = requests . post ( url , json = payload ) print ( response . text ) print ( response . status_code , response . reason ) Warning The first request to the web-service will be slow (~60s). This is because a large language model needs to be loaded into memory. Documentation for the Saber web-service API can be found here . We hope to provide a live version of the web-service soon!","title":"Web-service"},{"location":"quick_start/#pre-trained-models","text":"First, import Saber . This class coordinates training, annotation, saving and loading of models and datasets. In short, this is the interface to Saber. from saber.saber import Saber To load a pre-trained model, first create a Saber object saber = Saber () and then load the model of our choice saber . load ( PRGE ) Tip See Resources: Pre-trained models for pre-trained model names and details. You will need an internet connection to download a pre-trained model. To annotate text with the model, just call the Saber.annotate() method saber . annotate ( The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53. ) Warning The Saber.annotate() method will be slow the first time you call it (~60s). This is because a large language model needs to be loaded into memory.","title":"Pre-trained models"},{"location":"quick_start/#coreference-resolution","text":"Coreference occurs when two or more expressions in a text refer to the same person or thing, that is, they have the same referent . Take the following example: \" IL-6 supports tumour growth and metastasising in terminal patients, and it significantly engages in cancer cachexia (including anorexia) and depression associated with malignancy.\" Clearly, \" it \" referes to \" IL-6 \". If we do not resolve this coreference, then \" it \" will not be labeled as an entity and any relation or event it is mentioned in will not be extracted. Saber uses NeuralCoref , a state-of-the-art coreference resolution tool based on neural nets and built on top of Spacy . To use it, just supply the argument coref=True (which is False by default) to the Saber.annotate() method text = IL-6 supports tumour growth and metastasising in terminal patients, and it significantly engages in cancer cachexia (including anorexia) and depression associated with malignancy. # WITHOUT coreference resolution saber . annotate ( text , coref = False ) # WITH coreference resolution saber . annotate ( text , coref = True ) Note If you are using the web-service, simply supply \"coref\": true in your JSON payload to resolve coreferences. Saber currently takes the simplest possible approach: replace all coreference mentions with their referent, and then feed the resolved text to the model that identifies named entities.","title":"Coreference Resolution"},{"location":"quick_start/#grounding","text":"Grounding (sometimes called entity linking or normalization ) involves mapping each annotated entity to a unique identifier in an external resource such as a database or ontology. To ground entities in a call to Saber.annotate() , simply pass the argument ground=True saber . annotate ( The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53. , ground = True ) The grounding functionality is implemented by the EXTRACT 2.0 API . Note that you will need an internet connection or grounding will fail. Also note that Saber.annotate() will take slightly longer to return a response when ground=True (up to a few seconds). See Resources: Pre-trained models for a list of the the external resources each entity type (annotated by the pre-trained models) is grounded to. Note If you are using the web-service, simply supply \"ground\": true in your JSON payload to ground entities.","title":"Grounding"},{"location":"quick_start/#working-with-annotations","text":"The Saber.annotate() method returns a simple dict object ann = saber . annotate ( The phosphorylation of Hdm2 by MK2 promotes the ubiquitination of p53. ) which contains the keys title , text and ents title : contains the title of the article, if provided text : contains the text (which is minimally processed) the model was deployed on ents : contains a list of entities present in the text that were annotated by the model For example, to see all entities annotated by the model, call ann [ ents ]","title":"Working with annotations"},{"location":"quick_start/#converting-annotations-to-json","text":"The Saber.annotate() method returns a dict object, but can be converted to a JSON formatted string for ease-of-use in downstream applications import json # convert to json object json_ann = json . dumps ( ann ) # convert back to python dictionary ann = json . loads ( json_ann )","title":"Converting annotations to JSON"},{"location":"resources/","text":"Resources Saber is ready to go out-of-the box when using the web-service or a pre-trained model . However, if you plan on training you own models, you will need to provide a dataset (or datasets!) and, ideally, pre-trained word embeddings. Pre-trained models Pre-trained model names can be passed to Saber.load() (see Quick Start: Pre-trained Models ). Appending \"*-large\" to the model name (e.g. \"PRGE-large\" will download a much larger model, which should perform slightly better than the base model. Identifier Semantic Group Identified entity types Namespace CHED Chemicals Abbreviations and Acronyms, Molecular Formulas, Chemical database identifiers, IUPAC names, Trivial (common names of chemicals and trademark names), Family (chemical families with a defined structure) and Multiple (non-continuous mentions of chemicals in text) PubChem Compounds DISO Disorders Acquired Abnormality, Anatomical Abnormality, Cell or Molecular Dysfunction, Congenital Abnormality, Disease or Syndrome, Mental or Behavioral Dysfunction, Neoplastic Process, Pathologic Function, Sign or Symptom Disease Ontology LIVB Organisms Species, Taxa NCBI Taxonomy PRGE Genes and Gene Products Genes, Gene Products STRING Datasets Currently, Saber requires corpora to be in a CoNLL format with a BIO or IOBES tag scheme, e.g.: Selegiline B-CHED - O induced O postural B-DISO hypotension I-DISO ... Corpora in such a format are collected in here for convenience. Info Many of the corpora in the BIO and IOBES tag format were originally collected by Crichton et al ., 2017 , here . In this format, the first column contains each token of an input sentence, the last column contains the tokens tag, all columns are separated by tabs, and all sentences by a newline. Of course, not all corpora are distributed in the CoNLL format: Corpora in the Standoff format can be converted to CoNLL format using this tool. Corpora in PubTator format can be converted to Standoff first using this tool. Saber infers the \"training strategy\" based on the structure of the dataset folder: To use k-fold cross-validation, simply provide a train.* file in your dataset folder. E.g. . \u251c\u2500\u2500 NCBI_Disease \u2502 \u2514\u2500\u2500 train.tsv To use a train/valid/test strategy, provide train.* and test.* files in your dataset folder. Optionally, you can provide a valid.* file. If not provided, a random 10% of examples from train.* are used as the validation set. E.g. . \u251c\u2500\u2500 NCBI_Disease \u2502 \u251c\u2500\u2500 test.tsv \u2502 \u2514\u2500\u2500 train.tsv Word embeddings When training new models, you can (and should) provide your own pre-trained word embeddings with the pretrained_embeddings argument (either at the command line or in the configuration file). Saber expects all word embeddings to be in the word2vec file format. Pyysalo et al . 2013 provide word embeddings that work quite well in the biomedical domain, which can be downloaded here . Alternatively, from the command line call: # Replace this with a location you want to save the embeddings to $ mkdir path/to/word_embeddings # Note: this file is over 4GB $ wget http://evexdb.org/pmresources/vec-space-models/wikipedia-pubmed-and-PMC-w2v.bin -O path/to/word_embeddings To use these word embeddings with Saber, provide their path in the pretrained_embeddings argument (either in the config file or at the command line). Alternatively, pass their path to Saber.load_embeddings() . For example: from saber.saber import Saber saber = Saber () saber . load_dataset ( path/to/dataset ) # load the embeddings here saber . load_embeddings ( path/to/word_embeddings ) saber . build () saber . train () GloVe To use GloVe embeddings, just convert them to the word2vec format first: ( saber ) $ python from gensim.scripts.glove2word2vec import glove2word2vec glove_input_file = glove.txt word2vec_output_file = word2vec.txt glove2word2vec ( glove_input_file , word2vec_output_file )","title":"Resources"},{"location":"resources/#resources","text":"Saber is ready to go out-of-the box when using the web-service or a pre-trained model . However, if you plan on training you own models, you will need to provide a dataset (or datasets!) and, ideally, pre-trained word embeddings.","title":"Resources"},{"location":"resources/#pre-trained-models","text":"Pre-trained model names can be passed to Saber.load() (see Quick Start: Pre-trained Models ). Appending \"*-large\" to the model name (e.g. \"PRGE-large\" will download a much larger model, which should perform slightly better than the base model. Identifier Semantic Group Identified entity types Namespace CHED Chemicals Abbreviations and Acronyms, Molecular Formulas, Chemical database identifiers, IUPAC names, Trivial (common names of chemicals and trademark names), Family (chemical families with a defined structure) and Multiple (non-continuous mentions of chemicals in text) PubChem Compounds DISO Disorders Acquired Abnormality, Anatomical Abnormality, Cell or Molecular Dysfunction, Congenital Abnormality, Disease or Syndrome, Mental or Behavioral Dysfunction, Neoplastic Process, Pathologic Function, Sign or Symptom Disease Ontology LIVB Organisms Species, Taxa NCBI Taxonomy PRGE Genes and Gene Products Genes, Gene Products STRING","title":"Pre-trained models"},{"location":"resources/#datasets","text":"Currently, Saber requires corpora to be in a CoNLL format with a BIO or IOBES tag scheme, e.g.: Selegiline B-CHED - O induced O postural B-DISO hypotension I-DISO ... Corpora in such a format are collected in here for convenience. Info Many of the corpora in the BIO and IOBES tag format were originally collected by Crichton et al ., 2017 , here . In this format, the first column contains each token of an input sentence, the last column contains the tokens tag, all columns are separated by tabs, and all sentences by a newline. Of course, not all corpora are distributed in the CoNLL format: Corpora in the Standoff format can be converted to CoNLL format using this tool. Corpora in PubTator format can be converted to Standoff first using this tool. Saber infers the \"training strategy\" based on the structure of the dataset folder: To use k-fold cross-validation, simply provide a train.* file in your dataset folder. E.g. . \u251c\u2500\u2500 NCBI_Disease \u2502 \u2514\u2500\u2500 train.tsv To use a train/valid/test strategy, provide train.* and test.* files in your dataset folder. Optionally, you can provide a valid.* file. If not provided, a random 10% of examples from train.* are used as the validation set. E.g. . \u251c\u2500\u2500 NCBI_Disease \u2502 \u251c\u2500\u2500 test.tsv \u2502 \u2514\u2500\u2500 train.tsv","title":"Datasets"},{"location":"resources/#word-embeddings","text":"When training new models, you can (and should) provide your own pre-trained word embeddings with the pretrained_embeddings argument (either at the command line or in the configuration file). Saber expects all word embeddings to be in the word2vec file format. Pyysalo et al . 2013 provide word embeddings that work quite well in the biomedical domain, which can be downloaded here . Alternatively, from the command line call: # Replace this with a location you want to save the embeddings to $ mkdir path/to/word_embeddings # Note: this file is over 4GB $ wget http://evexdb.org/pmresources/vec-space-models/wikipedia-pubmed-and-PMC-w2v.bin -O path/to/word_embeddings To use these word embeddings with Saber, provide their path in the pretrained_embeddings argument (either in the config file or at the command line). Alternatively, pass their path to Saber.load_embeddings() . For example: from saber.saber import Saber saber = Saber () saber . load_dataset ( path/to/dataset ) # load the embeddings here saber . load_embeddings ( path/to/word_embeddings ) saber . build () saber . train ()","title":"Word embeddings"},{"location":"resources/#glove","text":"To use GloVe embeddings, just convert them to the word2vec format first: ( saber ) $ python from gensim.scripts.glove2word2vec import glove2word2vec glove_input_file = glove.txt word2vec_output_file = word2vec.txt glove2word2vec ( glove_input_file , word2vec_output_file )","title":"GloVe"}]}